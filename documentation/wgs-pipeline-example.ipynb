{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54f47672",
   "metadata": {},
   "source": [
    "# Parabricks FQ2BAM, HaplotypeCaller, and dbSNP: GPU-accelerated DNA WGS from reads to annotated variants\n",
    "\n",
    "---\n",
    "\n",
    "High-throughput DNA sequencing (NGS) transformed genomics from single-genome moonshots to population-scale studies. The first commercial “next-gen” platform (454 pyrosequencing, 2005) demonstrated massively parallel reads and precipitous cost declines, enabling efforts like the 1000 Genomes Project to chart common human variation and set de-facto file/analysis standards still used today.<sup>[1],[2]</sup> To move raw reads into analysis, the community coalesced on the **SAM/BAM** formats and a CPU toolchain centered on **BWA-MEM** for alignment and the **GATK Best Practices** for variant discovery—an architecture that remains the reference baseline for accuracy and interoperability.<sup>[3],[4],[5]</sup>  \n",
    "\n",
    "As cohorts grew from dozens to thousands of genomes, turnaround time and cost became bottlenecks. Systematic benchmarks showed that porting secondary analysis to **GPUs** yields order-of-magnitude runtime and cost reductions without sacrificing accuracy—paving the way for production suites like **NVIDIA Parabricks** that mirror gold-standard CPU pipelines while exploiting GPU parallelism. <sup>[12],[13]</sup>\n",
    "\n",
    "This notebook introduces three Parabricks building blocks that together take you **FASTQ ➜ BAM ➜ VCF ➜ annotated VCF** for whole-genome sequencing (WGS):\n",
    "\n",
    "---\n",
    "\n",
    "## 1) **Parabricks FQ2BAM** — GPU-accelerated read mapping & pre-processing\n",
    "\n",
    "**What it does.** FQ2BAM ingests raw FASTQs and emits a coordinate-sorted, duplicate-marked BAM aligned with **BWA-MEM**, optionally with Base Quality Score Recalibration (BQSR). Outputs match GATK-style expectations, so you can drop directly into downstream variant callers. In short: the canonical CPU steps, but GPU-fast. <sup>[8]</sup>\n",
    "\n",
    "**Why this step matters.** Aligners place reads onto the reference; SAM/BAM define how those alignments are stored; duplicate marking (Picard/MarkDuplicates-compatible) and BQSR standardize error profiles for robust variant calling. BWA-MEM and SAM/BAM are the community workhorses; Parabricks accelerates them while preserving parity with the baseline implementations.<sup>[3],[4],[5],[7],[14],[15],[16a],[16b]</sup>\n",
    "\n",
    "---\n",
    "\n",
    "## 2) **Parabricks HaplotypeCaller** — GPU-accelerated germline variant calling\n",
    "\n",
    "**What it does.** A GPU implementation of the **GATK HaplotypeCaller** workflow: it performs local de-novo assembly within “active regions” to jointly model SNPs and indels, producing high-quality gVCFs/VCFs consistent with GATK Best Practices—at a fraction of the CPU runtime.<sup>[9]</sup>\n",
    "\n",
    "**Where it comes from.** HaplotypeCaller emerged from the GATK framework that unified mapping, recalibration, and assembly-based variant discovery; the Best Practices continue to be stewarded and updated by the Broad Institute community (see also the 2020 *Genomics in the Cloud* reference). Parabricks maintains algorithmic equivalence while accelerating execution on GPUs.<sup>[5],[6],[10],[11]</sup>\n",
    "\n",
    "---\n",
    "\n",
    "## 3) **Parabricks dbSNP** — GPU-accelerated variant annotation\n",
    "\n",
    "**What it does.** Annotates your VCF against **dbSNP**, the long-running NCBI archive of known variants. The result is an annotated VCF with rsIDs and related metadata that downstream tools and databases recognize, produced quickly on GPUs.<sup>[10],[11]</sup>\n",
    "\n",
    "---\n",
    "\n",
    "## What you’ll do in this notebook\n",
    "\n",
    "1. **Run FQ2BAM** on example FASTQs to generate a sorted, duplicate-marked **BAM**.  \n",
    "2. **Call variants** with **HaplotypeCaller** to produce **gVCF/VCF**.  \n",
    "3. **Annotate variants** against **dbSNP** to add rsIDs and related fields.  \n",
    "4. (Optional) Compare GPU/CPU wall-time to quantify speedups and cost impacts on your hardware/cloud.  \n",
    "\n",
    "> Parabricks pipelines are designed to be **drop-in replacements** for the standard CPU workflows—same algorithms and outputs, dramatically faster on NVIDIA GPUs.<sup>[8],[12],[13]</sup>\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. Margulies M. *et al.* **[Genome sequencing in microfabricated high-density picolitre reactors][1]**. *Nature* (2005).  \n",
    "2. 1000 Genomes Project Consortium. **[A map of human genome variation from population-scale sequencing][2]**. *Nature* (2010).  \n",
    "3. Li H. *et al.* **[The Sequence Alignment/Map (SAM) format and SAMtools][3]**. *Bioinformatics* (2009).  \n",
    "4. Li H. **[Aligning sequence reads, clone sequences and assembly contigs with BWA-MEM][4]**. *arXiv* (2013).  \n",
    "5. Van der Auwera GA *et al.* **[From FastQ Data to High-Confidence Variant Calls: The Genome Analysis Toolkit Best Practices Pipeline][5]**. *Curr Protoc Bioinformatics* (2013).  \n",
    "6. **[HaplotypeCaller (overview & algorithm notes)][6]**. Broad Institute GATK Docs.  \n",
    "7. **Picard MarkDuplicates** — [documentation][7], [repository][7a]. Broad Institute.  \n",
    "8. **[Parabricks FQ2BAM Tutorial][8]**. NVIDIA Docs.  \n",
    "9. **[Parabricks HaplotypeCaller][9]**. NVIDIA Docs.  \n",
    "10. **[Parabricks dbSNP annotator][10]**. NVIDIA Docs.  \n",
    "11. Sherry ST *et al.* **[dbSNP: the NCBI database of genetic variation][11]**. *Nucleic Acids Res* (2001).  \n",
    "12. Taylor-Weiner A *et al.* **[Scaling computational genomics to millions of individuals with GPUs][12]**. *Genome Biology* (2019).  \n",
    "13. **Parabricks Documentation** — [overview][13], [output accuracy & CPU parity/benchmarks][13b]. NVIDIA Docs & Guides.  \n",
    "14. **[BWA GitHub repository][14]**.  \n",
    "15. **[HTS-specs (SAM/BAM/VCF) GitHub repository][15]**.  \n",
    "16. **Official specifications (PDFs):** [SAM v1][16a]; [VCF v4.3][16b].\n",
    "\n",
    "<!-- Link definitions -->\n",
    "[1]: https://www.nature.com/articles/nature03959\n",
    "[2]: https://www.nature.com/articles/nature09534\n",
    "[3]: https://pmc.ncbi.nlm.nih.gov/articles/PMC2723002/\n",
    "[4]: https://arxiv.org/abs/1303.3997\n",
    "[5]: https://pmc.ncbi.nlm.nih.gov/articles/PMC4243306/\n",
    "[6]: https://gatk.broadinstitute.org/hc/en-us/articles/360037225632-HaplotypeCaller\n",
    "[7]: https://gatk.broadinstitute.org/hc/en-us/articles/360037052812-MarkDuplicates-Picard\n",
    "[7a]: https://github.com/broadinstitute/picard\n",
    "[8]: https://docs.nvidia.com/clara/parabricks/latest/Tutorials/FQ2BAM_Tutorial.html\n",
    "[9]: https://docs.nvidia.com/clara/parabricks/latest/Documentation/ToolDocs/man_haplotypecaller.html\n",
    "[10]: https://docs.nvidia.com/clara/parabricks/latest/Documentation/ToolDocs/man_dbsnp.html\n",
    "[11]: https://academic.oup.com/nar/article/29/1/308/1116004\n",
    "[12]: https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1836-7\n",
    "[13]: https://docs.nvidia.com/clara/parabricks/4.5.1/Overview.html\n",
    "[13b]: https://docs.nvidia.com/clara/parabricks/latest/Documentation/ToolDocs/OutputAccuracyAndCompatibleCpuSoftwareVersions.html\n",
    "[14]: https://github.com/lh3/bwa\n",
    "[15]: https://github.com/samtools/hts-specs\n",
    "[16a]: https://samtools.github.io/hts-specs/SAMv1.pdf\n",
    "[16b]: https://samtools.github.io/hts-specs/VCFv4.3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3be8fed",
   "metadata": {},
   "source": [
    "## Download and preprocess datasets\n",
    "Download fasta, fastq, and vcf file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31da0f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"../datasets\"\n",
    "study_dir = f\"{base}/wgs\"\n",
    "# Create directories for files\n",
    "!mkdir $base\n",
    "!mkdir $study_dir\n",
    "# Fasta\n",
    "fasta_url = \"https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/405/GCF_000001405.26_GRCh38/GCF_000001405.26_GRCh38_genomic.fna.gz\"\n",
    "fasta_gz = \"../datasets/gws/GCF_000001405.26_GRCh38_genomic.fna.gz\"\n",
    "!wget -nc -P ../datasets/gws/ $fasta_url\n",
    "!gzip -d $fasta_gz\n",
    "# FastQ files\n",
    "fastq1_url = (\n",
    "    \"ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR133/022/ERR13301022/ERR13301022_1.fastq.gz\"\n",
    ")\n",
    "fastq2_url = (\n",
    "    \"ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR133/022/ERR13301022/ERR13301022_2.fastq.gz\"\n",
    ")\n",
    "!wget -nc -P $study_dir $fastq1_url\n",
    "!wget -nc -P $study_dir $fastq2_url\n",
    "# Known sites\n",
    "vcf_url = \"https://ftp.ncbi.nlm.nih.gov/snp/latest_release/VCF/GCF_000001405.25.gz\"\n",
    "vcf_md5_url = (\n",
    "    \"https://ftp.ncbi.nlm.nih.gov/snp/latest_release/VCF/GCF_000001405.25.gz.md5\"\n",
    ")\n",
    "vcf_tbi_url = (\n",
    "    \"https://ftp.ncbi.nlm.nih.gov/snp/latest_release/VCF/GCF_000001405.25.gz.tbi\"\n",
    ")\n",
    "vcf_tbi_md5_url = (\n",
    "    \"https://ftp.ncbi.nlm.nih.gov/snp/latest_release/VCF/GCF_000001405.25.gz.tbi.md5\"\n",
    ")\n",
    "knowns_sites = f\"{study_dir}/GCF_000001405.25.vcf.gz\"\n",
    "vcf_md5_name = f\"{study_dir}/GCF_000001405.25.vcf.gz.md5\"\n",
    "vcf_tbi_name = f\"{study_dir}/GCF_000001405.25.vcf.gz.tbi\"\n",
    "vcf_tbi_md5_name = f\"{study_dir}/GCF_000001405.25.vcf.gz.tbi.md5\"\n",
    "!wget -nc -O $knowns_sites $vcf_url\n",
    "!wget -nc -O $vcf_md5_name $vcf_md5_url\n",
    "!wget -nc -O $vcf_tbi_name $vcf_tbi_url\n",
    "!wget -nc -O $vcf_tbi_md5_name $vcf_tbi_md5_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803f023b",
   "metadata": {},
   "source": [
    "Index fasta file using bwa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1386baa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bwa_index] Pack FASTA... 10.72 sec\n",
      "[bwa_index] Construct BWT for the packed sequence...\n",
      "[BWTIncCreate] textLength=6418572210, availableWord=463634060\n",
      "[BWTIncConstructFromPacked] 10 iterations done. 99999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 20 iterations done. 199999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 30 iterations done. 299999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 40 iterations done. 399999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 50 iterations done. 499999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 60 iterations done. 599999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 70 iterations done. 699999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 80 iterations done. 799999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 90 iterations done. 899999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 100 iterations done. 999999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 110 iterations done. 1099999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 120 iterations done. 1199999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 130 iterations done. 1299999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 140 iterations done. 1399999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 150 iterations done. 1499999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 160 iterations done. 1599999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 170 iterations done. 1699999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 180 iterations done. 1799999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 190 iterations done. 1899999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 200 iterations done. 1999999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 210 iterations done. 2099999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 220 iterations done. 2199999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 230 iterations done. 2299999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 240 iterations done. 2399999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 250 iterations done. 2499999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 260 iterations done. 2599999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 270 iterations done. 2699999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 280 iterations done. 2799999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 290 iterations done. 2899999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 300 iterations done. 2999999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 310 iterations done. 3099999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 320 iterations done. 3199999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 330 iterations done. 3299999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 340 iterations done. 3399999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 350 iterations done. 3499999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 360 iterations done. 3599999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 370 iterations done. 3699999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 380 iterations done. 3799999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 390 iterations done. 3899999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 400 iterations done. 3999999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 410 iterations done. 4099999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 420 iterations done. 4199999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 430 iterations done. 4299999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 440 iterations done. 4399999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 450 iterations done. 4499999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 460 iterations done. 4599999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 470 iterations done. 4699999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 480 iterations done. 4799999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 490 iterations done. 4899999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 500 iterations done. 4999999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 510 iterations done. 5099999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 520 iterations done. 5199999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 530 iterations done. 5299999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 540 iterations done. 5399999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 550 iterations done. 5499999986 characters processed.\n",
      "[BWTIncConstructFromPacked] 560 iterations done. 5599999986 characters processed.\n"
     ]
    }
   ],
   "source": [
    "fasta_file = \"../datasets/wgs/GCF_000001405.26_GRCh38_genomic.fna\"\n",
    "# Index fasta with BWA\n",
    "!/opt/bwa/bwa index $fasta_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d3f341",
   "metadata": {},
   "source": [
    "Trimming fastq files to remove low quality reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16e1972",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq1_file = \"../datasets/wgs/ERR13301022_1.fastq.gz\"\n",
    "fastq2_file = \"../datasets/wgs/ERR13301022_2.fastq.gz\"\n",
    "trimmed_fastq1_file = \"../datasets/wgs/trimmed_ERR13301022_1.fastq.gz\"\n",
    "trimmed_fastq2_file = \"../datasets/wgs/trimmed_ERR13301022_2.fastq.gz\"\n",
    "# Preprocess fasta file with fastp\n",
    "!/opt/fastp \\\n",
    "    -i $fastq1_file -I $fastq2_file \\\n",
    "    -o $trimmed_fastq1_file -O $trimmed_fastq2_file \\\n",
    "    -w 16\n",
    "# Preprocess fasta\n",
    "!/opt/bwa/bwa index $fasta_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff76366",
   "metadata": {},
   "source": [
    "## Align reads to reference genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93bf62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bqsr_file = f\"{study_dir}/recal_file.txt\"\n",
    "bam_file = f\"{study_dir}/study.bam\"\n",
    "knowns_sites = f\"{study_dir}/GCF_000001405.25.vcf.gz\"\n",
    "\n",
    "!pbrun fq2bam \\\n",
    "    --in-fq $trimmed_fastq1_file $trimmed_fastq2_file \\\n",
    "    --knownSites $knowns_sites \\\n",
    "    --out-bam $bam_file \\\n",
    "    --ref $fasta_file \\\n",
    "    --out-recal-file $bqsr_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184e9b77",
   "metadata": {},
   "source": [
    "## Variant calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aafe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_file = f\"{study_dir}/ERR13301022.vcf\"\n",
    "\n",
    "!pbrun haplotypecaller \\\n",
    "    --ref $fasta_file \\\n",
    "    --in-bam $bam_file \\\n",
    "    --in-recal-file $bqsr_file \\\n",
    "    --out-variants $variant_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0634cd",
   "metadata": {},
   "source": [
    "## Annotates VCF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64266c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_variant_file = f\"{study_dir}/ERR13301022_annotated.vcf\"\n",
    "\n",
    "!pbrun dbsnp \\\n",
    "    --in-vcf $variant_file \\\n",
    "    --out-vcf $annotated_variant_file \\\n",
    "    --in-dbsnp-file $knowns_sites"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
